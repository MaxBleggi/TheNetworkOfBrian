{% extends "template.html" %}
{% block content %}

    <div class="about">
        <h1 class="body-title">About</h1>
        <p class="indented">
            The Network of Brian contains (and will contain more) neural networks.
            Each one is a variation on a convolution network.
            If you'd like to know more, go to the how it's made page.
            The name is a play on the movie, <i>The life of Brian</i> by Monty Python
            (seems appropriate considering it's implemented in Python, of which the language's name was coined from);
            furthermore, Brian is one character switch from Brain.
            Thusly, Network of Brian can be reasonably surmised to be a pseudonym for Brain (Neural) Network.
        </p>

        <p>
            It should also be stated that these networks are not optimized for optimal performance.
            I made them to experiment with and learn from not for production.
            Alterations can be made to improve their speeds.
            In fact, someone are created purely to compare their speed.
        </p>
    </div>

    <div class="what-is-it">
        <h1 class="body-title">What is a Neural Network?</h1>
        <p class="indented">
            Contrary to popular belief, the essence of what makes a neural network function is quite simple in theory.
            For the purposes of this explanation, I will oversimplify many things, but the intuition derived from this
            can act as a heuristic to understand more complex representations later on.
        </p>
        <p>
            Think of a simple math function, f(x). That simply takes an input, does some calculations
            then outputs a value. A neural network works in a similar way in that it can be thought of as
            a math formula but instead of a single variable, x, as input, it has thousands of variables.
        </p>
        <p>
            How a neural network learns can also be thought of in a similar way. Every time the word "learning" is used
            essentially just means finding the minimum of this function. We've all done this in high school with calculus.
            In fact, finding the minimum of a function is quite trivial. So why do we need a neural network to calculate
            these partial derivatives? Well lets look at an example.
        </p>

        <img class="poly-graph"src="static/img/poly-graph.png">

        <p>
            Finding the local (and even global) minima of this graph is easy. You can do it visually by seeing the lowest point.
            A neural networks graph, however, is magnitudes more complex. This example is in 2 - dimensions and only has one variable.
            A realistic graph would be in 3-dimensions or higher! It would also contain thousands of more variables. So calculating
            this, even with multi-variable calculus is practically impossible. Theoretically it is possible to calculate the
            global minimum of any given neural network with gradient descent; however, that is only theoretical. In fact, it
            is part of a class of problems called NP (non-polynomial deterministic). That is a different subject altogether though.
            For our purposes, just take for granted that finding the global minimum is practically impossible. In recent years, though,
            <a href="https://medium.com/syncedreview/global-minima-solution-for-neural-networks-22c1fd18616">there has been some interesting research on the subject by MIT and Carnegie Mellon </a>. So the best we can do is find a local minimum, but don't worry,
            because it turns out that all <a href="https://arxiv.org/abs/1412.0233">local minima are approximately equal</a> (not technically a true statement, but for our purposes, it fits well enough). So finding a minimum is usually good enough.
            In fact, you can see it with this network. The local minimum that this network stumbled down, has a peak of approximately
            95%. That's pretty good! I have, on the other hand, seen different networks process the same data with an accuracy of up
            to 98% and even 99%. Recognizing a hand written digit is fairly trivial compared to recognizing a face or God knows what
            else big tech companies have trained their in-house AI to do. That a large reason why everyone wants your data. To train AI.
            This simple network alone uses 50,000 images to learn.
        </p>

        <p>
            So that's the essence of a simple neural network. Now rewind the clock to the 1990's and this would be cutting edge, but
            more complex models, like deep learning for example, still rely on this concept of a simple neural network.
        </p>

    </div>


    <section class="performance-">
        <h1 class="def-header">Performance</h1>
        <p class="indented">
            Currently, this page holds two implementations of the same network. The first one is an intuitive, albiet naive,
            implementation of gradient descent. It simply iterates over all the values and calculates the gradient of the curve.
            The reason this implementation is ultimately naive is because linear algebra exists. This is where the second
            implementation comes into frame. This fully matrix-based one collects all the values into a vector performs some basic linear
            algebra. Why this is significant will become apparent below.
        </p>

        <p>
            When it comes to accuracy, you can clearly see that the difference is non-existant.
            This is because they both perform the operation, just in a different manner. Looking at the plot, you can
            see that the accuracy reaches an asymptotic maximum because we've found the local minimum of the function.
            There are almost certainly more minimums that are perhaps more optimal, but as stated prior, finding them
            tant amount to finding the global minimum. So a fools errand.
        </p>


        <div id="error-vs-epoch" class = "plots"></div>

        <p>
            An Epoch is exactly that, a length of time. In this case it is one complete run through of a
            50,000 item training set of images. So you can imagine how this program might take a while.
            It has to calculate 781 pixels (28x28 px image)
            50,000 times per epoch.
        </p>

        <div id="epoch-vs-time" class = "plots"></div>

        <p>
            Analysing the speed of the networks, now reveals why the iterative implementation is naive.
            The iterative version has a fairly linear run time as expected, but the interesting part is the
            curve of the matrix based network. The library I used for linear algebra in numpy, a Python library.
            I don't know how it is implemented exatly but we can extrapolate that it's runtime is bounded by what I believe
            is log(n) or nlog(n) by the shape of the curve.
        </p>
    </section>

{% endblock %}